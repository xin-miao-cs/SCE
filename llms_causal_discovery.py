import math
import tqdm
import torch
import itertools

from conceptnet_utils import get_related_concepts


def locate_start_index(seq_list, sub_seq_list):
    """Locate the start index of a subsequence in a sequence."""
    for index in range(len(seq_list)):
        if seq_list[index: index+len(sub_seq_list)] == sub_seq_list:
            return index


def compute_probability(tokenizer, model, query_concept, device="cuda:0"):
    """Compute the probability of query concept generated by LLMs."""
    prompt = f"The most likely appear is {query_concept}"

    pos_assist = f"placeholder {query_concept}"
    pos_start = len(tokenizer.encode("placeholder test")) - 1
    query_ids = tokenizer.encode(pos_assist)[pos_start:]

    prompt_ids = tokenizer.encode(prompt)
    query_start = locate_start_index(prompt_ids, query_ids)

    with torch.no_grad():
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        outputs = model(**inputs)
        scores = outputs.logits[0]
        query_scores = scores[query_start-1: query_start-1 + len(query_ids)]
        query_probability_distributions = query_scores.softmax(dim=-1)

        assert len(query_ids) == len(query_probability_distributions)

        query_probability = float(1.0)
        for num_id, query_id in enumerate(query_ids):
            query_probability *= query_probability_distributions[num_id][query_id].item()

        return query_probability


def compute_conditional_probability(tokenizer, model, query_concept, given_concept1, given_concept2=None, device="cuda:0"):
    """Compute the probability of LLMs generating query concept based on given concept(s)."""
    if given_concept2 is None:
        prompt_type = 0
    else:
        prompt_type = 1

    prompts = {0: f"When {given_concept1} appears, the next most likely to appear is {query_concept}",
               1: (f"When {given_concept1} appears, the next most likely to appear is {given_concept2},"
                   f" the next most likely to appear is {query_concept}")}
    prompt = prompts[prompt_type]

    pos_assist = f"placeholder {query_concept}"
    pos_start = len(tokenizer.encode("placeholder test")) - 1
    query_ids = tokenizer.encode(pos_assist)[pos_start:]

    prompt_ids = tokenizer.encode(prompt)
    query_start = locate_start_index(prompt_ids, query_ids)

    with torch.no_grad():
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        outputs = model(**inputs)
        scores = outputs.logits[0]
        query_scores = scores[query_start-1: query_start-1 + len(query_ids)]
        query_probability_distributions = query_scores.softmax(dim=-1)

        assert len(query_ids) == len(query_probability_distributions)

        query_probability = float(1.0)
        for num_id, query_id in enumerate(query_ids):
            query_probability *= query_probability_distributions[num_id][query_id].item()

        return query_probability


def compute_joint_probability(tokenizer, model, query_concept1, query_concept2, query_concept3=None, device="cuda:0"):
    """Compute joint probability of LLMs generating several query concepts simultaneously."""
    if query_concept3 is None:
        prompt_type = 0
    else:
        prompt_type = 1

    prompts = {0: f"The most likely to appear simultaneously with {query_concept1} is {query_concept2}",
               1: (f"The most likely to appear simultaneously with {query_concept1} are {query_concept2}"
                   f" and {query_concept3}")}
    prompt = prompts[prompt_type]

    pos_assists = {0: [f"placeholder {query_concept1}", f"placeholder {query_concept2}"],
                   1: [f"placeholder {query_concept1}", f"placeholder {query_concept2}",
                       f"placeholder {query_concept3}"]}
    pos_assists = pos_assists[prompt_type]

    queries_ids = list()
    pos_start = len(tokenizer.encode("placeholder test")) - 1
    for pos_assist in pos_assists:
        queries_ids.append(tokenizer.encode(pos_assist)[pos_start:])

    prompt_ids = tokenizer.encode(prompt)
    query_starts = list()
    for query_ids in queries_ids:
        query_starts.append(locate_start_index(prompt_ids, query_ids))

    assert len(queries_ids) == len(query_starts)

    with torch.no_grad():
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        outputs = model(**inputs)
        scores = outputs.logits[0]

        queries_probability = float(1.0)
        for num_query, query_ids in enumerate(queries_ids):
            query_start = query_starts[num_query]
            query_scores = scores[query_start-1: query_start-1 + len(query_ids)]
            query_probability_distributions = query_scores.softmax(dim=-1)

            assert len(query_ids) == len(query_probability_distributions)

            query_probability = float(1.0)
            for num_id, query_id in enumerate(query_ids):
                query_probability *= query_probability_distributions[num_id][query_id].item()

            queries_probability *= query_probability

        return queries_probability


def is_approximately_equal(num1, num2, tolerance):
    """Check if two numbers are equal within an acceptable tolerance."""
    if num1 == 0:
        scale_num1 = 1e-100
    else:
        scale_num1 = math.log10(num1)

    if num2 == 0:
        scale_num2 = 1e-100
    else:
        scale_num2 = math.log10(num2)

    return abs(scale_num1 - scale_num2) <= tolerance


def is_correlate(tokenizer, model, concept1, concept2, strength, device="cuda:0"):
    """Check if two concepts are statistically correlated for LLMs.
       Strength indicates correlated degree, larger means more correlated.
    """
    probability_concept1 = compute_probability(tokenizer, model, concept1, device=device)
    probability_concept1_on_concept2 = compute_conditional_probability(tokenizer, model, concept1, concept2, device=device)

    if not is_approximately_equal(probability_concept1, probability_concept1_on_concept2, strength):
        if probability_concept1_on_concept2 > probability_concept1:
            return True
    return False


def is_independent(tokenizer, model, concept1, concept2, tolerance, device="cuda:0"):
    """Check if two concepts are statistically independent for LLMs.
       Tolerance indicates acceptable fluctuation, lower means more independent.
    """
    probability_concept1 = compute_probability(tokenizer, model, concept1, device=device)
    probability_concept1_on_concept2 = compute_conditional_probability(tokenizer, model, concept1, concept2, device=device)

    if is_approximately_equal(probability_concept1, probability_concept1_on_concept2, tolerance):
        return True
    else:
        if probability_concept1 > probability_concept1_on_concept2:
            return True
        else:
            return False


def is_conditional_correlate(tokenizer, model, concept1, concept2, given_concept, tolerance, device="cuda:0"):
    """Check if two concepts are statistically correlated conditioned on another concept for LLMs.
       Note that we assume concept1 is correlated with the conditional concept in this function.
       Tolerance indicates acceptable fluctuation, lower means more correlated.
    """
    probability_concept1_on_given_concept = compute_conditional_probability(tokenizer, model, concept1, given_concept, device=device)
    probability_concept1_on_given_concept_concept2 = compute_conditional_probability(tokenizer, model, concept1, given_concept, concept2, device=device)

    if is_approximately_equal(probability_concept1_on_given_concept, probability_concept1_on_given_concept_concept2, tolerance):
        return True
    else:
        if probability_concept1_on_given_concept < probability_concept1_on_given_concept_concept2:
            return True
        else:
            return False


def combine_elements(element_list, size=2):
    """Combine the elements in a list."""
    combinations = list(itertools.combinations(element_list, size))
    return combinations


def discover_cause_concepts(tokenizer, model, effect_concept, strength, tolerance, device="cuda:0"):
    """Discover the cause concepts that driver LLms to generate the given effect concept."""
    cause_concepts = list()

    correlated_concepts = list()
    for related_concept in get_related_concepts(effect_concept):
        if is_correlate(tokenizer, model, effect_concept, related_concept, strength, device=device):
            correlated_concepts.append(related_concept)

    correlated_tuples = combine_elements(correlated_concepts)
    for correlated_tuple in tqdm.tqdm(correlated_tuples):
        correlated_concept1, correlated_concept2 = correlated_tuple
        if correlated_concept1 not in cause_concepts or correlated_concept2 not in cause_concepts:
            if is_independent(tokenizer, model, correlated_concept1, correlated_concept2, tolerance, device=device):
                if is_conditional_correlate(tokenizer, model, correlated_concept1, correlated_concept2, effect_concept, tolerance, device=device):
                    if correlated_concept1 not in cause_concepts:
                        cause_concepts.append(correlated_concept1)
                    if correlated_concept2 not in cause_concepts:
                        cause_concepts.append(correlated_concept2)

    return cause_concepts
